{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from scipy.stats import zscore\n",
    "\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Date</th>\n",
       "      <th>TeamA</th>\n",
       "      <th>TeamB</th>\n",
       "      <th>Result</th>\n",
       "      <th>S</th>\n",
       "      <th>Team A Kills</th>\n",
       "      <th>Team A Errors</th>\n",
       "      <th>Team A Total Attacks</th>\n",
       "      <th>Team A Hit Pct</th>\n",
       "      <th>...</th>\n",
       "      <th>Team B Hit Pct</th>\n",
       "      <th>Team B Assists</th>\n",
       "      <th>Team B Aces</th>\n",
       "      <th>Team B SErr</th>\n",
       "      <th>Team B Digs</th>\n",
       "      <th>Team B RErr</th>\n",
       "      <th>Team B Block Solos</th>\n",
       "      <th>Team B Block Assists</th>\n",
       "      <th>Team B BErr</th>\n",
       "      <th>Team B PTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>8302019</td>\n",
       "      <td>USC Upstate</td>\n",
       "      <td>Virginia Tech</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>33</td>\n",
       "      <td>21</td>\n",
       "      <td>112</td>\n",
       "      <td>0.107</td>\n",
       "      <td>...</td>\n",
       "      <td>0.234</td>\n",
       "      <td>48</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>51</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>8302019</td>\n",
       "      <td>USC Upstate</td>\n",
       "      <td>Northwestern</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16</td>\n",
       "      <td>24</td>\n",
       "      <td>89</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.349</td>\n",
       "      <td>41</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>9072019</td>\n",
       "      <td>USC Upstate</td>\n",
       "      <td>Ga. Southern</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>37</td>\n",
       "      <td>20</td>\n",
       "      <td>117</td>\n",
       "      <td>0.145</td>\n",
       "      <td>...</td>\n",
       "      <td>0.239</td>\n",
       "      <td>39</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>49</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>9132019</td>\n",
       "      <td>USC Upstate</td>\n",
       "      <td>Norfolk St.</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>50</td>\n",
       "      <td>27</td>\n",
       "      <td>164</td>\n",
       "      <td>0.140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114</td>\n",
       "      <td>47</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>67</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>70.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9142019</td>\n",
       "      <td>USC Upstate</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30</td>\n",
       "      <td>17</td>\n",
       "      <td>112</td>\n",
       "      <td>0.116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.216</td>\n",
       "      <td>44</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     Date        TeamA              TeamB  Result    S  \\\n",
       "0           0  8302019  USC Upstate      Virginia Tech       0  4.0   \n",
       "1           1  8302019  USC Upstate       Northwestern       0  3.0   \n",
       "2           2  9072019  USC Upstate       Ga. Southern       0  3.0   \n",
       "3           3  9132019  USC Upstate        Norfolk St.       1  5.0   \n",
       "4           4  9142019  USC Upstate  George Washington       0  3.0   \n",
       "\n",
       "   Team A Kills  Team A Errors  Team A Total Attacks  Team A Hit Pct  ...  \\\n",
       "0            33             21                   112           0.107  ...   \n",
       "1            16             24                    89          -0.090  ...   \n",
       "2            37             20                   117           0.145  ...   \n",
       "3            50             27                   164           0.140  ...   \n",
       "4            30             17                   112           0.116  ...   \n",
       "\n",
       "   Team B Hit Pct  Team B Assists  Team B Aces  Team B SErr  Team B Digs  \\\n",
       "0           0.234              48           13           12           51   \n",
       "1           0.349              41            6            7           42   \n",
       "2           0.239              39            4            5           49   \n",
       "3           0.114              47           11           15           67   \n",
       "4           0.216              44            3            1           60   \n",
       "\n",
       "   Team B RErr  Team B Block Solos  Team B Block Assists  Team B BErr  \\\n",
       "0            4                   4                    20            2   \n",
       "1            1                   2                    20            0   \n",
       "2            2                   1                     6            1   \n",
       "3            9                   1                    17            1   \n",
       "4            2                   1                     6            6   \n",
       "\n",
       "   Team B PTS  \n",
       "0         0.0  \n",
       "1         0.0  \n",
       "2        50.0  \n",
       "3        70.5  \n",
       "4        52.0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"C:/Users/gabriel.ferreira_ext/Documents/ENG4502/team_v_team.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando colunas para obter eficiência geral\n",
    "\n",
    "df[\"Team A Total Errors\"] = df['Team A Errors'] + df['Team A SErr'] + df['Team A BErr'] + df['Team A RErr']\n",
    "df[\"Team B Total Errors\"] = df['Team B Errors'] + df['Team B SErr'] + df['Team B BErr'] + df['Team B RErr']\n",
    "df[\"Team A Total Rounds\"] = df['Team A PTS'] + df['Team A Total Errors']\n",
    "df[\"Team B Total Rounds\"] = df['Team B PTS'] + df['Team B Total Errors']\n",
    "df[\"Team A Efficiency Rate\"] = df[\"Team A PTS\"] - df[\"Team A Total Errors\"] / df[\"Team A Total Rounds\"]\n",
    "df[\"Team B Efficiency Rate\"] = df[\"Team B PTS\"] - df[\"Team B Total Errors\"] / df[\"Team B Total Rounds\"]\n",
    "\n",
    "# Gerando colunas para obter eficiência de bloqueio\n",
    "\n",
    "df[\"Team A Total Blocks\"] = df['Team A Block Solos'] + df['Team A Block Assists']*0.5 + df['Team A BErr']\n",
    "df[\"Team B Total Blocks\"] = df['Team B Block Solos'] + df['Team B Block Assists']*0.5 + df['Team B BErr']\n",
    "df[\"Team A Success Blocks\"]= df['Team A Block Solos'] + df['Team A Block Assists']*0.5\n",
    "df[\"Team B Success Blocks\"]= df['Team B Block Solos'] + df['Team B Block Assists']*0.5\n",
    "df[\"Team A Block Efficiency\"] = df[\"Team A Success Blocks\"] / df[\"Team A Total Blocks\"]\n",
    "df[\"Team B Block Efficiency\"] = df[\"Team B Success Blocks\"] / df[\"Team B Total Blocks\"]\n",
    "\n",
    "# Gerando colunas para obter a eficiência de defesa\n",
    "\n",
    "df[\"Team A Total Digs\"] = df['Team A Digs'] + df['Team A RErr']\n",
    "df[\"Team B Total Digs\"] = df['Team B Digs'] + df['Team B RErr']\n",
    "df[\"Team A Efficiency Digs\"] = df[\"Team A Digs\"] / df[\"Team A Total Digs\"]\n",
    "df[\"Team B Efficiency Digs\"] = df[\"Team B Digs\"]  / df[\"Team B Total Digs\"]\n",
    "\n",
    "ind_remocao = df[\"Team A Total Errors\"].loc[df[\"Team A Total Errors\"] == 0].index\n",
    "df.drop(ind_remocao, inplace=True)\n",
    "\n",
    "# ind_remocao = df[\"Team A Block Efficiency\"].loc[df[\"Team A Block Efficiency\"] == 0].index\n",
    "# df.drop(ind_remocao, inplace=True)\n",
    "# ind_remocao = df[\"Team B Block Efficiency\"].loc[df[\"Team B Block Efficiency\"] == 0].index\n",
    "# df.drop(ind_remocao, inplace=True)\n",
    "\n",
    "ind_remocao = df[\"Team B Total Blocks\"].loc[df[\"Team B Total Blocks\"] == 0].index\n",
    "df.drop(ind_remocao, inplace=True)\n",
    "ind_remocao = df[\"Team A Total Blocks\"].loc[df[\"Team A Total Blocks\"] == 0].index\n",
    "df.drop(ind_remocao, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um dicionário para mapear os IDs dos times\n",
    "team_ids = {team: idx for idx, team in enumerate(set(df[\"TeamA\"]).union(df[\"TeamB\"]))}\n",
    "\n",
    "# Adicionando colunas de IDs de time\n",
    "df[\"TeamA_ID\"] = df[\"TeamA\"].map(team_ids)\n",
    "df[\"TeamB_ID\"] = df[\"TeamB\"].map(team_ids)\n",
    "\n",
    "# Criando a coluna de ID da partida, garantindo que os jogos reversos tenham o mesmo ID\n",
    "df[\"Match_ID\"] = df.apply(lambda row: (row[\"Date\"], tuple(sorted([row[\"TeamA_ID\"], row[\"TeamB_ID\"]]))), axis=1)\n",
    "\n",
    "# Convertendo a combinação (data, time A ID, time B ID) em um identificador numérico único\n",
    "df[\"Match_ID\"] = df[\"Match_ID\"].apply(lambda x: hash(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normaliza(df):\n",
    "    scaler = MinMaxScaler()\n",
    "    df_norm = df\n",
    "    df_norm = scaler.fit_transform(df)\n",
    "    return df_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier(df):\n",
    "    z_scores = zscore(df, nan_policy='omit')\n",
    "    df_norm_clean = df[(abs(z_scores) < 3).all(axis=1)]\n",
    "    return df_norm_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas_bloq = ['Team A Block Solos', 'Team A Block Assists', 'Team A BErr','Team A Block Efficiency','Team A Total Blocks', 'Team A Success Blocks']\n",
    "colunas_bloq_fe = ['Team A Block Solos', 'Team A Block Assists', 'Team A BErr']\n",
    "\n",
    "df_bloq = df[colunas_bloq]\n",
    "df_bloq_fe = df[colunas_bloq_fe]\n",
    "\n",
    "lst_df = [df_bloq,df_bloq_fe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import DBSCAN, AgglomerativeClustering, KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from scipy.stats import zscore\n",
    "from itertools import product\n",
    "\n",
    "# Função para gerar cenários\n",
    "def generate_scenarios():\n",
    "    normalizations = [True, False]\n",
    "    outliers = [True, False]\n",
    "    dataframes = ['df_bloq', 'df_bloq_fe']\n",
    "    models = ['dbscan', 'kmeans', 'hierarchical']\n",
    "    scenarios = list(product(normalizations, outliers, dataframes, models))\n",
    "    return scenarios\n",
    "\n",
    "# Função para aplicar modelo\n",
    "def apply_model(model, data, params):\n",
    "    if model == 'dbscan':\n",
    "        cluster = DBSCAN(\n",
    "            eps=params.get('eps', 0.5),\n",
    "            min_samples=params.get('min_samples', 5),\n",
    "            metric=params.get('metric', 'euclidean'),\n",
    "            algorithm=params.get('algorithm', 'auto')\n",
    "        )\n",
    "    elif model == 'kmeans':\n",
    "        cluster = KMeans(\n",
    "            n_clusters=params.get('n_clusters', 3),\n",
    "            init=params.get('init', 'k-means++'),\n",
    "            max_iter=params.get('max_iter', 300),\n",
    "            random_state=42\n",
    "        )\n",
    "    elif model == 'hierarchical':\n",
    "        cluster = AgglomerativeClustering(\n",
    "            n_clusters=params.get('n_clusters', 2),\n",
    "            linkage=params.get('linkage', 'ward'),\n",
    "            affinity=params.get('affinity', 'euclidean')\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Modelo {model} não suportado.\")\n",
    "    labels = cluster.fit_predict(data)\n",
    "    return labels\n",
    "\n",
    "# Função para avaliar modelo\n",
    "def evaluate_model(data, labels):\n",
    "    if len(np.unique(labels)) > 1:  # Evitar erro de métricas com apenas 1 cluster\n",
    "        silhouette = silhouette_score(data, labels)\n",
    "        davies_bouldin = davies_bouldin_score(data, labels)\n",
    "    else:\n",
    "        silhouette = None\n",
    "        davies_bouldin = None\n",
    "    return silhouette, davies_bouldin\n",
    "\n",
    "# Função principal\n",
    "def execute_scenarios(df, output_file='resultados_cenarios.csv'):\n",
    "    scenarios = generate_scenarios()\n",
    "    results = []\n",
    "\n",
    "    # Abrir ou criar o arquivo CSV de resultados\n",
    "    try:\n",
    "        existing_results = pd.read_csv(output_file)\n",
    "        processed_scenarios = set(existing_results['Scenario ID'])\n",
    "    except FileNotFoundError:\n",
    "        processed_scenarios = set()\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write('Scenario ID,Normalization,Remove Outliers,DataFrame Type,Model,Params,Silhouette Score,Davies-Bouldin Index\\n')\n",
    "\n",
    "    for scenario_id, scenario in enumerate(scenarios):\n",
    "        # Identificar o ID do cenário\n",
    "        if scenario_id in processed_scenarios:\n",
    "            continue  # Pular cenários já processados\n",
    "\n",
    "        norm, remove_outliers, dataframe_type, model_type = scenario\n",
    "\n",
    "        # Selecionar o dataframe\n",
    "        if dataframe_type == 'df_bloq':\n",
    "            data = df[colunas_bloq].copy()\n",
    "        else:\n",
    "            data = df[colunas_bloq_fe].copy()\n",
    "\n",
    "        # Aplicar normalização e remoção de outliers\n",
    "        if norm:\n",
    "            data = normaliza(data)\n",
    "        if remove_outliers:\n",
    "            data = outlier(data)\n",
    "\n",
    "        # Testar diferentes hiperparâmetros\n",
    "        if model_type == 'dbscan':\n",
    "            param_grid = [\n",
    "                {'eps': eps, 'min_samples': min_samples, 'metric': metric, 'algorithm': algorithm}\n",
    "                for eps in np.linspace(0.02, 0.07, 15)\n",
    "                for min_samples in range(3, 10)\n",
    "                for metric in ['euclidean', 'manhattan', 'cosine']\n",
    "                for algorithm in ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "            ]\n",
    "        elif model_type == 'kmeans':\n",
    "            param_grid = [\n",
    "                {'n_clusters': n_clusters, 'init': init, 'max_iter': max_iter}\n",
    "                for n_clusters in range(2, 10)\n",
    "                for init in ['k-means++', 'random']\n",
    "                for max_iter in [100, 300, 500]\n",
    "            ]\n",
    "        elif model_type == 'hierarchical':\n",
    "            param_grid = [\n",
    "                {'n_clusters': n_clusters, 'linkage': linkage, 'affinity': affinity}\n",
    "                for n_clusters in range(2, 10)\n",
    "                for linkage in ['ward', 'complete', 'average', 'single']\n",
    "                for affinity in ['euclidean', 'manhattan', 'cosine']\n",
    "            ]\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # Iterar sobre combinações de parâmetros\n",
    "        for params in param_grid:\n",
    "            try:\n",
    "                labels = apply_model(model_type, data, params)\n",
    "                silhouette, davies_bouldin = evaluate_model(data, labels)\n",
    "\n",
    "                # Criar linha de resultado\n",
    "                result = {\n",
    "                    'Scenario ID': scenario_id,\n",
    "                    'Normalization': norm,\n",
    "                    'Remove Outliers': remove_outliers,\n",
    "                    'DataFrame Type': dataframe_type,\n",
    "                    'Model': model_type,\n",
    "                    'Params': params,\n",
    "                    'Silhouette Score': silhouette,\n",
    "                    'Davies-Bouldin Index': davies_bouldin\n",
    "                }\n",
    "                results.append(result)\n",
    "\n",
    "                # Adicionar ao CSV\n",
    "                pd.DataFrame([result]).to_csv(output_file, mode='a', index=False, header=False)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Erro no cenário {scenario} com parâmetros {params}: {e}\")\n",
    "\n",
    "    # Retornar todos os resultados acumulados como DataFrame\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 8 fields in line 45, saw 15\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mexecute_scenarios\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m results\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresultados_cenarios.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[14], line 62\u001b[0m, in \u001b[0;36mexecute_scenarios\u001b[1;34m(df, output_file)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Abrir ou criar o arquivo CSV de resultados\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 62\u001b[0m     existing_results \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m     processed_scenarios \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(existing_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScenario ID\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\gabriel.ferreira_ext\\Documents\\ENG4502\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gabriel.ferreira_ext\\Documents\\ENG4502\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gabriel.ferreira_ext\\Documents\\ENG4502\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\gabriel.ferreira_ext\\Documents\\ENG4502\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 8 fields in line 45, saw 15\n"
     ]
    }
   ],
   "source": [
    "results = execute_scenarios(df)\n",
    "results.to_csv('resultados_cenarios.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
